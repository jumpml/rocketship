#  JumpML Rocketship - Neural Network Inference with Audio Processing
#  
#  Copyright 2020-2024 JUMPML LLC
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
#  convert_model.py
# 
#  Helps to convert Pytorch/JumpML (ptj) model file format to C by creating
#  1. NN inference & Preprocessing config header: /include/signalsifter_config.h
#  2. NN weights src file: /src/signalsifter_weights.c
#  3. Float vs. fixed-point NN inference testvector: test/testss/signalsifter_testvector.h (Unit Test)
#
 
import argparse
import torch
import torch.nn as nn
import numpy as np
import re
# import librosa
from model.simple_gru import GRU3
import os.path
import math
from datetime import date
from run_prediction import preproc
import librosa
from utils.utils import load_model_and_config, initialize_config

def printHeader(f, include_files=["signalsifter.h"], fName="signalsifter_weights.c",
                author="convert_model.py", autogen_str="from a Pytorch model and config file"):
    """
    Generates source header text with default header includes
        config.h, nn.h, nn_data.h
    """
    current_year = date.today().year
    f.write(f'//\n//  {fName}\n//\n//  Created by {author} \n//  Â© {current_year} JumpML\n')
    f.write(f'/*This file is automatically generated {autogen_str}*/\n')
    #f.write('#ifdef HAVE_CONFIG_H\n#include "config.h"\n#endif\n\n')

    for file in include_files:
        if "<" in file:
            f.write(f'#include {file}\n')
        else:
            f.write(f'#include "{file}"\n')



def find_activation(named_module_list, module_name):
    """
    Input: A list of named modules and the module_name
    Output: Extracts activation correponding to module_name

    Model Definition Conventions:
        the activation function is defined in module_name_activation 

    Supported activation functions:
        Tanh, ReLU, Sigmoid, Linear/Unknown

    Called by ModuleInfoGen. 
    """
    activation_name = "UNKNOWN"
    for name, module in named_module_list:
        if module_name + "_activation" == name:
            if isinstance(module, nn.Tanh):
                activation_name = "TANH"
            elif isinstance(module, nn.ReLU):
                activation_name = "RELU"
            elif isinstance(module, nn.Sigmoid):
                activation_name = "SIGMOID"
            else:
                activation_name = "UNKNOWN"

    return activation_name

def ModuleInfoGen(model):
    """
    Inputs: Pytorch model object (which has named_modules and parameters)
    Outputs: returns a tuple with module info
        (name, module_type, activation_type, input_dim, output_dim )
    
    Supported module types:
        Supports nn.Linear, nn.GRU modules

    Called by printModelStruct. 
    """
    # (name, module_type, activation_type, input_dim, output_dim )
    named_module_list = list(model.named_modules())
    for name, module in named_module_list:
        if isinstance(module, nn.Linear):
            activation_type = find_activation(named_module_list, name)
            yield (modify_param_name(name), "Linear", activation_type, module.in_features, module.out_features)
        elif isinstance(module, nn.GRU):
            activation_type = "TANH"  # Non-negotiable TANH in PyTorch
            yield (modify_param_name(name), "GRU", activation_type, module.input_size, module.hidden_size)


def printModelStruct(f, model, structType="RNNModel", instanceName="ss_model"):
    """
    input: file handle f, pytorch model object, optional structName and postfix
    output: None

    Prints model data structure, which includes module_structs of the modules that
    make up the model.
    """
    f.write(f'const {structType} {instanceName} = {{\n')
    


    for name, module_type, act_type, input_size, output_size in ModuleInfoGen(model):
        if module_type == "GRU":
            f.write('    {},\n'.format(output_size))
        elif module_type == "Linear":
            f.write('    {},\n'.format(output_size))
        else:
            print(
                f'Unknown layer: {name} with shape: {input_size}x{output_size}')

        f.write('    &{},\n'.format(name))

    f.write('};\n')

def print_system_info_header(text):
    l = len(text) + 8
    header_text = l*'=' 
    print(f"\n{header_text}")
    print(f"{text}")
    print(f"{header_text}")

def printModuleStructs(f, model):
    """    
    input: file handle f, pytorch model object
    output: None

    Prints module data structures (GRU and Linear) that are part of the model. 
    The module data structure includes names of parameter arrays such as 
    bias, weights and activation function type
    """
    print_system_info_header("Neural Network Architecture")
    for name, module_type, act_type, input_size, output_size in ModuleInfoGen(model):
        print(f'{name}, {module_type}, {act_type}, {input_size} x {output_size}')
        if module_type == 'GRU':
            f.write(
                f'static const GRULayer {name} = {{\n   {name}_bias, \n   {name}_recurrent_bias, \n   {name}_weights, \n   {name}_recurrent_weights, \n   {input_size}, {output_size}, ACTIVATION_{act_type}\n}};\n\n')
        elif module_type == 'Linear':
            f.write(
                f'static const LinearLayer {name} = {{\n   {name}_bias,\n   {name}_weights,\n   {input_size}, {output_size}, ACTIVATION_{act_type}\n}};\n\n')
        else:
            print(f'Unknown layer: {name} with type: {module_type}')
    print("\n\n")


def convert_datatype(w, datatype, nFracBits=7):
    scaleFac = 2 ** nFracBits
    if datatype == 'float':
        w = np.float32(w)
    elif datatype == 'int8_t':
        w = np.round(scaleFac*w)
        w = w.astype(int)
        w = np.maximum(-128, np.minimum(127, w))
    elif datatype == 'int16_t':
        w = np.round(scaleFac*w)
        w = w.astype(int)
        w = np.maximum(-32768, np.minimum(32767, w))


    return w

def printVector(f, vector, name, datatype='float', nFracBits=7):
    v = convert_datatype(np.reshape(vector, (-1)), datatype, nFracBits=nFracBits)
    f.write(f'DSP_RWDATA_IN_DRAM DSP_ALIGN16 static const {datatype} {name}[{len(v)}] = {{\n   ')
    for i in range(0, len(v)-1):
        if datatype == 'float':
            f.write(f'{v[i]:.6}f,')
        else:
            f.write(f'{v[i]},')
        if (i % 8 == 7):
            f.write("\n   ")
        else:
            f.write(" ")

    if datatype == 'float':
        f.write(f'{v[len(v)-1]:.6}f,')
    else:
        f.write(f'{v[len(v)-1]}')
    f.write('\n};\n\n')
    return

def modify_param_name(name):
    # Step 1: Replace all . with _
    modified_name = name.replace('.', '_')
    # Step 2: GRU specific modifications
    modified_name = modified_name.replace('weight_ih_l0', 'weights')
    modified_name = modified_name.replace('weight_hh_l0', 'recurrent_weights')
    modified_name = modified_name.replace('bias_ih_l0', 'bias')
    modified_name = modified_name.replace('bias_hh_l0', 'recurrent_bias')

    # Step 3: if we find a singular "weight", replace with "weights"
    if re.findall(r'weight\b', modified_name):
       modified_name = modified_name.replace('weight', 'weights')

    #print(f'{name} --> {modified_name}')
    return modified_name

def create_model_header_file(fname="./include/signalsifter_config.h", w_datatype='int8_t', b_datatype='int8_t',
                    w_fracBits=7, b_fracBits=7, H=352, io_size=128, model_name="SIGNALSIFTER",
                    fft_size=256, hop_length=128, logmag_epsilon=1e-4, act_wordlen=16, input_intBits=6):
    header_name = fname.split(".h")[0]
    header_name = os.path.basename(header_name)
    # header_fname =  "./include/" + header_name + ".h"
    f = open(fname, 'w')
    printHeader(f,include_files=[], fName=fname)
    f.write(f'#ifndef {header_name}_h\n')
    f.write(f'#define {header_name}_h\n')
    f.write(f"#include <stdint.h>\n")

    f.write(f'\n#define WAB_FRAC_BITS  ({w_fracBits})')
    f.write(f'\n#define INPUT_NUM_FRAC_BITS  ({act_wordlen - input_intBits - 1})')
    f.write(f'\n#define GRU_NUM_FRAC_BITS  ({act_wordlen - 1})')
    f.write(f'\n#define LIN_NUM_FRAC_BITS  ({act_wordlen - 1})')
    f.write(f'\n#define INPUTLAYER_SHIFT_RIGHT  ((INPUT_NUM_FRAC_BITS) + WAB_FRAC_BITS - (GRU_NUM_FRAC_BITS))')
    f.write(f'\ntypedef {w_datatype} nnWeight;\n#define WEIGHTS_SCALE (1.f/{2**w_fracBits})')
    f.write(f'\ntypedef {b_datatype} nnBias;\n#define BIAS_SCALE (1.f/{2**b_fracBits})\n')
    
    f.write(f'\n#define GRU_STATE_SIZE {H}')
    f.write(f'\n#define MAX_NEURONS {max(H, io_size)}')
    f.write(f'\n#define IO_SIZE {io_size}')
    f.write(f'\n#define FFT_SIZE {fft_size}')
    f.write(f'\n#define HOP_LENGTH {hop_length}')
    f.write(f'\n#define FRAME_SIZE {hop_length}')   #Application Frame Size. ToDo: Make this an Application config setting.
    f.write(f'\n#define NUM_BINS {int(fft_size/2) + 1}\n')

    f.write(f'\n#define LOGMAG_EPSILON {logmag_epsilon}')
    f.write(f'\n#define LOGMAG_EPSILON_Q15 (int16_t)(LOGMAG_EPSILON * 32767.0f)\n\n')
    f.write(f'#endif\n')

def get_fracBits(datatype):
    
    if datatype == 'int8_t':
        fracBits = 7
    elif datatype == 'int16_t':
        fracBits = 15
    elif datatype == 'float':
        fracBits = 0
    elif datatype == 'int32_t':
        fracBits = 31
    else:
        print("Unknown datatype")

    return fracBits




def dump_model_to_Cfile(model, src_fname="/tmp/nn_data.c", hdr_fname=None, w_datatype = 'int8_t', b_datatype="int16_t", 
                        H=128, io_size=161, fft_size=320, hop_length=160, logmag_epsilon=1e-4, input_intBits=0):
    f = open(src_fname, 'w')
    printHeader(f)

    w_fracBits = get_fracBits(w_datatype)
    b_fracBits = get_fracBits(b_datatype)

    create_model_header_file(fname=hdr_fname, w_datatype=w_datatype, b_datatype=b_datatype, w_fracBits=w_fracBits, b_fracBits=b_fracBits, 
                            H=H, io_size=io_size, fft_size=fft_size, hop_length=hop_length, logmag_epsilon=logmag_epsilon, input_intBits=input_intBits)
    
    for name, param in model.named_parameters():
        mod_name = modify_param_name(name)
        if param.requires_grad:
            # print(f'{mod_name}: {param.data.shape} \n')
            if mod_name and "bias" in mod_name:
                printVector(f, param.data.numpy(), mod_name, b_datatype)
            elif mod_name and "weight" in mod_name:
                printVector(f, param.data.numpy(), mod_name, w_datatype)
            else:
                print("oops")

    print_system_info_header("System Processing Flow")
    print(f"{hop_length} samples --> logMagSpec(FFT{fft_size}) --> NN Masker(IO={io_size}) --> ISTFT(FFT{fft_size}) --> {hop_length} samples")

    printModuleStructs(f, model)
    printModelStruct(f, model, structType="SignalSifterModel", instanceName="ss_model")
    f.close()

def dump_testvectors_Cfile(input, output, state_in, state_out, fname="test/testss/signalsifter_testvector.h", input_intBits=0):
    f = open(fname, 'w')
    datatypes = ['float', 'int16_t']
    postfix = ['', '_S16']
    printHeader(f, include_files="",fName=fname)
    

    for idx, dt in enumerate(datatypes):
        nFracBits = get_fracBits(dt)
        printVector(f, input.data.numpy(), f'test_input{postfix[idx]}', datatype=dt, nFracBits=nFracBits-input_intBits)
        printVector(f, output.data.numpy(), f'test_output{postfix[idx]}', datatype=dt, nFracBits=nFracBits)
        printVector(f, state_in[0].data.numpy(), f'test_state_in1{postfix[idx]}', datatype=dt, nFracBits=nFracBits)
        printVector(f, state_in[1].data.numpy(), f'test_state_in2{postfix[idx]}', datatype=dt, nFracBits=nFracBits)
        printVector(f, state_in[2].data.numpy(), f'test_state_in3{postfix[idx]}', datatype=dt, nFracBits=nFracBits)
        printVector(f, state_out[0].data.numpy(), f'test_state_out1{postfix[idx]}', datatype=dt, nFracBits=nFracBits)
        printVector(f, state_out[1].data.numpy(), f'test_state_out2{postfix[idx]}', datatype=dt, nFracBits=nFracBits)
        printVector(f, state_out[2].data.numpy(), f'test_state_out3{postfix[idx]}', datatype=dt, nFracBits=nFracBits)

    
def inference(model, nn_features, N, io_size=128):
    B,T,F = nn_features.shape
    h0 =  torch.zeros( 1, model.hidden_size[0], dtype=torch.float32)
    state_ =[h0, h0, h0]
    with torch.no_grad():
        for idx in range(N):
            state_in = state_
            ipt = nn_features[:,idx,:io_size] 
            out, state_out = model(ipt, state_in)
            state_ = state_out

    return ipt, out, state_in, state_out 

def generate_test_vectors(model, config, enable_testvector=True):
    if enable_testvector:
        y, _ = librosa.load("data/test_vector.wav", sr=16000)
        preproc = initialize_config(config["preprocessing"])
        (noisy_mag, noisy_phase, nn_features_tensor) = preproc.process(y)
        ipt, out, state_in, state_out = inference(model, nn_features_tensor, 100, io_size=io_size)
        print_system_info_header(f"Max val {torch.max(nn_features_tensor)} \t Min val = {torch.min(nn_features_tensor)}")

    else:
        ipt = torch.rand(1, 1, model.input_size, dtype=torch.float32)
        ipt = 20*torch.log10(logmag_epsilon + torch.tensor([10.869651, 16.671688, 26.74527, 33.615658, 33.473988, 27.30435, 20.0569, 15.559971, 12.970183, 11.602915, 11.486061, 11.619814, 11.750951, 11.694953, 10.745853, 9.114418, 7.9566035, 7.086113, 6.0092897, 5.1131635, 4.458161, 4.0274725, 3.4854794, 2.757676, 2.4103115, 1.9966626, 1.1067314, 0.24194857, 0.27826765, 0.643473, 0.73862195, 0.5553532, 0.9257371, 1.2276006, 1.2931978, 1.4213613, 1.597105, 1.807468, 1.7115953, 1.3121465, 1.3970919, 1.5908898, 1.3832546, 1.3054293, 1.439988, 1.380313, 1.320168, 1.3674905, 1.3300865, 1.2359328, 1.2235013, 1.2872403, 1.2971729, 1.2790587, 1.2544004, 1.2064505, 1.1639596, 1.0956968, 0.9661235, 0.8403754, 0.8049353, 0.7664929, 0.6860457, 0.6247753, 0.51401734, 0.4327611, 0.3806892, 0.24829106, 0.4782541, 0.6006703, 0.53517514, 0.58825415, 0.6124979, 0.56295013, 0.6441737, 0.72400266, 0.79937303, 0.8965566, 0.8969054, 0.7727565, 0.61489075, 0.6346722, 0.7403714, 0.7460339, 0.7132762, 0.69163364, 0.64030623, 0.56419665, 0.48843664, 0.5366717, 0.6023088, 0.60178584, 0.5906987, 0.52865714, 0.51420754, 0.5628825, 0.5432606, 0.49792212, 0.5045553, 0.5234665, 0.50975037, 0.46311402, 0.41829136, 0.4093598, 0.40069383, 0.36333385, 0.3279804, 0.33399597, 0.36369646, 0.37688333, 0.35912573, 0.35330653, 0.3711064, 0.39472657, 0.4445981, 0.4651349, 0.4835688, 0.50805837, 0.4781384, 0.48001722, 0.5014009, 0.4818455, 0.48933104, 0.51101243, 0.4945479, 0.47264197, 0.4481584, 0.39673552, 0.4021186, 0.43152967, 0.38316366, 0.35522723, 0.37549475, 0.3545854, 0.3426673, 0.3324708, 0.31249693, 0.331516, 0.33506835, 0.32602382, 0.33439767, 0.324918, 0.32124168, 0.34077123, 0.3352492, 0.31267986, 0.32700986, 0.3490128, 0.35510468, 0.35690668, 0.34015277, 0.30949694, 0.28459585, 0.25389484, 0.2139383, 0.17479344, 0.14366016, 0.12711209, 0.11935056, 0.11536923, 0.11424923]))
        ipt = ipt[:model.input_size]
        ipt = torch.unsqueeze(torch.unsqueeze(ipt,0),0)
        h0 =  torch.rand(1, 1, model.hidden_size[0], dtype=torch.float32)
        state_in =[h0, h0, h0]
        with torch.no_grad():
            out, state_out = model(ipt, state_in)
    
    return ipt, out, state_in, state_out

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Convert PyTorch Model to run in C")
    parser.add_argument("-m", "--model_file", required=False,
                        type=str, help="Pytorch JumpML model (.ptj) file", default='models/pretrained_models/jumpmlnr_pro.ptj')
    args = parser.parse_args()

    model, configuration = load_model_and_config(args.model_file)
    
    torch.manual_seed(configuration["seed"])  
    np.random.seed(configuration["seed"])

    io_size = configuration["model"]["args"]["io_size"]
    fft_size = configuration["preprocessing"]["args"]["n_fft"]
    hop_length = configuration["preprocessing"]["args"]["hop_length"]
    hidden_size = configuration["model"]["args"]["hidden_size"][0]
    logmag_epsilon = configuration["preprocessing"]["args"]["logmag_epsilon"]

    input_intBits = math.ceil(math.log2(abs(10*math.log10(logmag_epsilon))))
    
    (ipt, out, state_in, state_out) = generate_test_vectors(model, configuration)
    dump_testvectors_Cfile(ipt, out, state_in, state_out, input_intBits=input_intBits)
    # dump_model_to_Cfile(model, fname='./src/signalsifter_weights.c', datatype='float')
    # dump_model_to_Cfile(model, fname='./src/signalsifter_weights.c', datatype='int16_t')
    
    dump_model_to_Cfile(model, src_fname='./src/signalsifter_weights.c', hdr_fname='./include/signalsifter_config.h', 
                        w_datatype='int8_t', b_datatype='int8_t', H=hidden_size, io_size=io_size, fft_size=fft_size, 
                        hop_length=hop_length, logmag_epsilon=logmag_epsilon, input_intBits=input_intBits)
    # dump_model_to_Cfile(model, fname='./src/signalsifter_weights.c', w_datatype='float', b_datatype='float')

